{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "In this notebook we will explore autoencoder models. These are models in which the inputs are *encoded* to some intermediate representation before this representation is then *decoded* to try to reconstruct the inputs. They are example of a model which uses an unsupervised training method and are both interesting as a model in their own right and as a method for pre-training useful representations to use in supervised tasks such as classification. Autoencoders were covered as a pre-training method as additional material in the sixth lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Linear autoencoders\n",
    "\n",
    "For the first exercise we will consider training a simple 'contractive' autoencoder - that is one in which the hidden representation is smaller in dimension than the input and the objective is to minimise the mean squared error between the original inputs and reconstructed inputs. To begin with we will consider models in which the encoder and decoder are both simple affine transformations.\n",
    "\n",
    "When training an autoencoder the target outputs for the model are the original inputs. A simple way to integrate this in to our `mlp` framework is to define a new data provider inheriting from a base data provider (e.g. `MNISTDataProvider`) which overrides the `next` method to return the inputs batch as both inputs and targets to the model. A data provider of this form has been provided for you in `mlp.data_providers` as `MNISTAutoencoderDataProvider`.\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "Use this data provider to train an autoencoder model with a 50 dimensional hidden representation and both encoder and decoder defined by affine transformations. You should use a sum of squared differences error and a basic gradient descent learning rule with learning rate 0.01. Initialise the biases to zero and use a uniform Glorot initialisation for both layers weights. Train the model for 25 epochs with a batch size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import mlp.layers as layers\n",
    "import mlp.models as models\n",
    "import mlp.optimisers as optimisers\n",
    "import mlp.errors as errors\n",
    "import mlp.learning_rules as learning_rules\n",
    "import mlp.data_providers as data_providers\n",
    "import mlp.initialisers as initialisers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Create the model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function defined in the cell below (from the first lab notebook), plot a batch of the original images and the autoencoder reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch_of_images(img_batch, fig_size=(3, 3), num_rows=None):\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    batch_size, im_height, im_width = img_batch.shape\n",
    "    if num_rows is None:\n",
    "        # calculate grid dimensions to give square(ish) grid\n",
    "        num_rows = int(batch_size**0.5)\n",
    "    num_cols = int(batch_size * 1. / num_rows)\n",
    "    if num_rows * num_cols < batch_size:\n",
    "        num_cols += 1\n",
    "    # intialise empty array to tile image grid into\n",
    "    tiled = np.zeros((im_height * num_rows, im_width * num_cols))\n",
    "    # iterate over images in batch + indexes within batch\n",
    "    for i, img in enumerate(img_batch):\n",
    "        # calculate grid row and column indices\n",
    "        r, c = i % num_rows, i // num_rows\n",
    "        tiled[r * im_height:(r + 1) * im_height, \n",
    "              c * im_height:(c + 1) * im_height] = img\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(tiled, cmap='Greys', vmin=0., vmax=1.)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the generated images and original images using the above function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional extension: principle components analysis\n",
    "\n",
    "*This section is provided for the interest of those also sitting MLPR or otherwise already familiar with eigendecompositions and PCA. Feel free to skip over if this doesn't apply to you (or even if it does).*\n",
    "\n",
    "For a linear (affine) contractive autoencoder model trained with a sum of squared differences error function there is an analytic solution for the optimal model parameters corresponding to [principle components analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "If we have a training dataset of $N$ $D$-dimensional vectors $\\left\\lbrace \\boldsymbol{x}^{(n)} \\right\\rbrace_{n=1}^N$, then we can calculate the empiricial mean and covariance of the training data using\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{\\mu} = \\frac{1}{N} \\sum_{n=1}^N \\left[ \\boldsymbol{x}^{(n)} \\right]\n",
    "  \\qquad\n",
    "  \\text{and}\n",
    "  \\qquad\n",
    "  \\mathbf{\\Sigma} = \\frac{1}{N} \n",
    "  \\sum_{n=1}^N \\left[ \n",
    "    \\left(\\boldsymbol{x}^{(n)} - \\boldsymbol{\\mu} \\right)\n",
    "    \\left(\\boldsymbol{x}^{(n)} - \\boldsymbol{\\mu} \\right)^{\\rm T}\n",
    "  \\right].\n",
    "\\end{equation}\n",
    "\n",
    "We can then calculate an [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the covariance matrix \n",
    "\\begin{equation}\n",
    "  \\mathbf{\\Sigma} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{\\rm T}\n",
    "  \\qquad\n",
    "  \\mathbf{Q} = \\left[ \n",
    "  \\begin{array}{cccc}\n",
    "  \\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n",
    "  \\boldsymbol{q}_1 & \\boldsymbol{q}_2 & \\cdots & \\boldsymbol{q}_D \\\\\n",
    "  \\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n",
    "  \\end{array}\n",
    "  \\right]\n",
    "  \\qquad\n",
    "  \\mathbf{\\Lambda} = \\left[ \n",
    "  \\begin{array}{cccc} \n",
    "  \\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "  0 & \\lambda_2 & \\cdots & \\vdots \\\\\n",
    "  \\vdots & \\vdots & \\ddots & 0 \\\\ \n",
    "  0 & 0 & \\cdots & \\lambda_D \\\\ \n",
    "  \\end{array} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "with $\\mathbf{Q}$ an orthogonal matrix, $\\mathbf{Q}\\mathbf{Q}^{\\rm T} = \\mathbf{I}$, with columns $\\left\\lbrace \\boldsymbol{q}_d \\right\\rbrace_{d=1}^D$ corresponding to the eigenvectors of $\\mathbf{\\Sigma}$ and $\\mathbf{\\Lambda}$ a diagonal matrix with diagonal elements $\\left\\lbrace \\lambda_d \\right\\rbrace_{d=1}^D$ the corresponding eigenvalues of $\\mathbf{\\Sigma}$. \n",
    "\n",
    "Assuming the eigenvalues are ordered such that $\\lambda_1 < \\lambda_2 < \\dots < \\lambda_D$ then the top $K$ principle components of the inputs (eigenvectors with largest eigenvalues) correspond to $\\left\\lbrace \\boldsymbol{q}_d \\right\\rbrace_{d=D + 1 - K}^D$. If we define a $D \\times K$ matrix $\\mathbf{V} = \\left[ \\boldsymbol{q}_{D + 1 - K} ~ \\boldsymbol{q}_{D + 2 - K} ~\\cdots~ \\boldsymbol{q}_D \\right]$ then we can find the projections of a (mean normalised) input vector on to the selected $K$ principle components as $\\boldsymbol{h} = \\mathbf{V}^{\\rm T}\\left( \\boldsymbol{x} - \\boldsymbol{\\mu}\\right)$. We can then use these principle component projections to form a reconstruction of the original input just in terms of the $K$ top principle components using $\\boldsymbol{r} = \\mathbf{V} \\boldsymbol{h} + \\boldsymbol{\\mu}$. We can see that this is just a sequence of two affine transformations and so is directly analagous to a model with two affine layers and with $K$ dimensional outputs of the first layer / inputs to second.\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "The function defined in the cell below will calculate the PCA solution for a set of input vectors and a defined number of components $K$. Use it to calculate the top 50 principle components of the MNIST training data. Use the returned matrix and mean vector to calculate the PCA based reconstructions of a batch of 50 MNIST images and use the `show_batch_of_images` function to plot both the original and reconstructed inputs alongside each other. Also calculate the sum of squared differences error for the PCA solution on the MNIST training set and compare to the figure you got by gradient descent based training above. Will the gradient based training produce the same hidden representations as the PCA solution if it is trained to convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_parameters(inputs, num_components=50):\n",
    "    mean = inputs.mean(0)\n",
    "    inputs_zm = inputs - mean[None, :]\n",
    "    covar = np.einsum('ij,ik', inputs_zm, inputs_zm)\n",
    "    eigvals, eigvecs = np.linalg.eigh(covar)\n",
    "    return eigvecs[:, -num_components:], mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO caclulate PCA reconstructions of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO calculate sum of squared differences error for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Non-linear autoencoders\n",
    "\n",
    "Those who did the extension in the previous exercise will have just seen that for an autoencoder with both linear / affine encoder and decoders, there is an analytic solution for the parameters which minimise a sum of squared differences error.\n",
    "\n",
    "In general the advantage of using gradient-based training methods is that it allows us to use non-linear models for which there is no analytic solution for the optimal parameters. The hope is the use of non-linear transformations between the affine transformation layers will increase the representational power of the model (a sequence of affine transformations applied without any interleaving non-linear operations can always be represented by a single affine transformation).\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "Train a contractive autoencoder with an initial affine layer (output dimension again 50) followed by a rectified linear layer, then an affine transformation projecting to outputs of same dimension as the original inputs, and finally a logistic sigmoid layer at the output. As the only layers with parameters are the two affine layers which have the same dimensions as in the fully affine model above, the overall model here has the same number of parameters as previously.\n",
    "\n",
    "Again train for 25 epochs with 50 training examples per batch and use a uniform Glorot initialisation for the weights, and zero biases initialisation. Use our implementation of the 'Adam' adaptive moments learning rule (available in `mlp.learning_rules` as `AdamLearningRule`) rather than basic gradient descent here (the adaptivity helps deal with the varying appropriate scale of updates induced by the non-linear transformations in this model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO initialize and train the non-linear autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot batches of the inputs and reconstructed inputs for this non-linear contractive autoencoder model and compare to the corresponding plots for the linear models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot images of the original and reconstructed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Denoising autoencoders\n",
    "\n",
    "So far we have just considered autoencoders that try to reconstruct the input vector fed into them via some intermediate lower-dimensional 'contracted' representation. The contraction is important as if we were to mantain the input dimensionality in all layers of the model a trivial optima for the model to learn would be to apply an identity transformation at each layer.\n",
    "\n",
    "It can be desirable for the intermediate hidden representation to be robust to noise in the input. The intuition is that this will force the model to learn to maintain the 'important structure' in the input in the hidden representation (that needed to reconstruct the input). This also removes the requirement to have a contracted hidden representation (as the model can no longer simply learn to apply an identity transformation) though in practice we will still often use a lower-dimensional hidden representation as we believe there is a certain level of redundancy in the input data and so the important structure can be represented with a lower dimensional representation.\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "\n",
    "Create a new data provider object which adds to noise to the inputs to an autoencoder in each batch it returns. There are various different ways you could introduce noise. The three suggested in the lecture slides are\n",
    "\n",
    "  * *Gaussian*: add independent, zero-mean Gaussian noise of a fixed standard-deviation to each dimension of the input vectors.\n",
    "  * *Masking*: generate a random binary mask and perform an elementwise multiplication with each input (forcing some subset of the values to zero).\n",
    "  * *Salt and pepper*: select a random subset of values in each input and randomly assign either zero or one to them.\n",
    "  \n",
    "You should choose one of these noising schemes to implement. It may help to know that the base `DataProvider` object already has access to a random number generator object as its `self.rng` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDenoisingAutoencoderDataProvider(data_providers.MNISTDataProvider):\n",
    "    \"\"\"Simple wrapper data provider for training a denoising autoencoder on MNIST.\"\"\"\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        return NotImplementedError(\"TODO implement this using a noising scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented your chosen scheme, use the new data provider object to train a denoising autoencoder with the same model architecture as in exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO train the model using the new data provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `show_batch_of_images` function from above to visualise a batch of noisy inputs from your data provider implementation and the denoised reconstructions from your trained denoising autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot images of the original and reconstructed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Using an autoencoder as an initialisation for supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise we will use the first layer of an autoencoder for MNIST digit images as a layer within a multiple layer model trained to do digit classification. The intuition behind pretraining methods like this is that the hidden representations learnt by an autoencoder should be a more useful representation for training a classifier than the raw pixel values themselves. We could fix the parameters in the layers taken from the autoencoder but generally we can get better performance by letting the whole model be trained end-to-end on the supervised training task, with the learnt autoencoder parameters in this case acting as a potentially more intelligent initialisation than randomly sampling the parameters which can help ease some of the optimisation issues encountered due to poor initialisation of a model.\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "You can either use one of the autoencoder models you trained in the previous exercises, or train a new autoencoder model for specifically for this exercise. Create a new model object (instance of `mlp.models.MultipleLayerModel`) in which the first layer(s) of the list of layer passed to the model constructor are the trained first layer(s) from your autoencoder model (these can be accessed via the `layers` attribute which is a list of all the layers in a model). Add any additional layers you wish to the pretrained layers - at the very least you will need to add an output layer with output dimension 10 to allow the model to be used to predict class labels. Train this new model on the original MNIST image, digit labels pairs with a cross entropy error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO intialize a new classification model using a trained encoder, then train the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "In this section we will construct an Autoencoder with PyTorch. We will use the MNIST dataset and a simple linear encoder and decoder. We will keep the hidden representation relatively small (50 dimensions) and use the MSE loss function and the Adam optimizer. The Adam optimizer is a good choice for this task since it is an adaptive learning rate method and we do not need to tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aim to encode our images in a smaller dimension space and then decode them back to the original space, our output dimension should be the same as the input dimension.\n",
    "\n",
    "The size of the `hidden_dim` is subject to choice. If we choose a small `hidden_dim` we will have a more compressed representation of our data. However, if we choose a large `hidden_dim` we will have a more accurate reconstruction of our data, albeit with a higher risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set training run hyperparameters\n",
    "batch_size = 128  # number of data points in a batch\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 100  # number of training epochs to perform\n",
    "stats_interval = 5  # epoch interval between recording and printing stats\n",
    "\n",
    "input_dim = 1*28*28 # images are grayscale and 28 x 28 pixels\n",
    "output_dim = 1*28*28 # the output is the same size as the input image\n",
    "hidden_dim = 50 # the size of the latent vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will only use the [ToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) transform. This will convert our images to PyTorch tensors and scale the pixel values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "valid_size=0.2 # Leave 20% of training set as validation set\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "np.random.shuffle(indices) # Shuffle indices in-place\n",
    "train_idx, valid_idx = indices[split:], indices[:split] # Split indices into training and validation sets\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a linear layer for the encoder and a linear layer for the decoder. The encoder will take as input a 784 dimensional vector and output a 50 dimensional vector. The decoder will take as input a 50 dimensional vector and output a 784 dimensional vector. Although autoencoders usually have a mirror structure around the bottleneck, this is not a requirement, and you can experiment with different structures.\n",
    "\n",
    "We will use the [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html) activation function for the encoder. The Tanh activation function is a scaled version of the sigmoid activation function and outputs values between -1 and 1. This is useful since our latent space will be normalised to have values between -1 and 1 and will make it easy for us to sample from the latent space to generate new images.\n",
    "\n",
    "We will use the [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation function for the decoder. The Sigmoid activation function outputs values between 0 and 1.\n",
    "\n",
    "*Why do we want to choose the Sigmoid activation for the output layer? (Think about the range of our input values.)*\n",
    "\n",
    "Our model is composed of two `nn.Sequential` modules, one for the encoder and one for the decoder. This translates to two forward passes, one for the encoder to reduce dimensionality of out inputs and one for the decoder to reconstruct our original image from the latent space encoding. This is a more practical choice, as, after training the model, it allows us to use the encoder and the decoder separately. For example, we can use the encoder to encode images in the latent space and then use the decoder to generate new images by sampling the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self,input_dim, output_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, output_dim),\n",
    "        nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "  def forward(self,x):\n",
    "    z = self.encoder(x)\n",
    "    x = self.decoder(z)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(input_dim, output_dim, hidden_dim).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) loss function here. \n",
    "\n",
    "*Why would this loss function would be the best choice for this task?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss() # Mean squared error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the autoencoder is learning in an unsupervised manner, we no longer need the labels for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the loss values over training\n",
    "train_loss = [] \n",
    "valid_loss = []\n",
    "\n",
    "# Train model\n",
    "for i in range(num_epochs+1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        x = x.view(x.size(0), -1) # flatten images into vectors\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(x)\n",
    "        E_value = loss(y, x)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        E_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        batch_loss.append(E_value.item())\n",
    "    \n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "    for batch_idx, (x, _) in enumerate(valid_loader):\n",
    "        x = x.to(device)\n",
    "        x = x.view(x.size(0), -1) # flatten images into vectors\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(x)\n",
    "        E_value = loss(y, x)\n",
    "        \n",
    "        # Logging\n",
    "        batch_loss.append(E_value.item())\n",
    "    \n",
    "    valid_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    if i % stats_interval == 0:\n",
    "            print('Epoch: {} \\tError(train): {:.6f} \\tError(valid): {:.6f} '.format(\n",
    "                i, train_loss[-1], valid_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in the validation and training set error over training.\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(train_loss, label='Error(train)')\n",
    "ax_1.plot(valid_loss, label='Error(valid)')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can display a batch of images and their reconstructions. We can see that the reconstructions are not perfect, but they are quite good. We can also see that the reconstructions are a bit blurry. This is because we are using a linear decoder. We can improve the quality of the reconstructions by using a non-linear decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "x_flatten = x.view(x.size(0), -1)\n",
    "\n",
    "# Pass data through model\n",
    "with torch.no_grad():\n",
    "    y = model(x_flatten).detach().cpu().numpy()\n",
    "    y = y.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "# Plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "# Display input images on top row and reconstructions on bottom one\n",
    "for images, row in zip([x.detach().cpu(), y], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoders are generative models and can create *new* images by sampling the latent space. We can sample the latent space by taking a random sample from a uniform distribution on the interval $[0,1)$ and normalising it to the interval $[-1,1)$. We can then pass the sampled values through the decoder to generate new images.\n",
    "\n",
    "*Why do we need to normalise the sample in such a way?*\n",
    "\n",
    "The generative power of autoencoders is quite small compared to other generative models, such as GANs and VAEs. However, they are much easier to train and can be used as a good starting point for more complex generative models.\n",
    "\n",
    "*Why is their generative power small? (Think about the way the data they are trained on and they way they are trained.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2, 10\n",
    "sample_encodings = (torch.rand(rows*cols, hidden_dim).to(device) - 0.5) * 2 # normalise sample to match the encoding space [-1,1)\n",
    "with torch.no_grad():\n",
    "    generated_imgs = model.decoder(sample_encodings).detach().cpu().numpy()\n",
    "    generated_imgs = generated_imgs.reshape(rows*cols, 1, 28, 28)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "for images, row in zip([generated_imgs, generated_imgs[-10:]], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)   \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
